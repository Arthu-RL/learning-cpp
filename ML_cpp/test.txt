// Structure to hold the model
struct MLP {
    // Weights for input to hidden layer
    float w1[2][2];
    // Weights for hidden to output layer
    float w2[2];
    // Biases for hidden layer
    float b1[2];
    // Bias for output layer
    float b2;

    // Constructor initializes weights and biases randomly
    MLP() {
        // Initialize random seed
        srand(time(0));
        
        // Initialize weights and biases between -1.0 and 1.0
        for (int i = 0; i < 2; ++i) {
            for (int j = 0; j < 2; ++j) {
                w1[i][j] = 2.0f * ((float)rand() / RAND_MAX) - 1.0f;
            }
            w2[i] = 2.0f * ((float)rand() / RAND_MAX) - 1.0f;
            b1[i] = 2.0f * ((float)rand() / RAND_MAX) - 1.0f;
        }
        b2 = 2.0f * ((float)rand() / RAND_MAX) - 1.0f;
    }

    // Forward pass
    void forward(float x[2], float &y, float h[2]) {
        for (int i = 0; i < 2; ++i) {
            h[i] = sigmoid(x[0] * w1[i][0] + x[1] * w1[i][1] + b1[i]);
        }
        y = sigmoid(h[0] * w2[0] + h[1] * w2[1] + b2);
    }

    // Training function
    void train(std::vector<std::vector<float>> &X, std::vector<float> &Y, int epochs, float learningRate) {
        for (int epoch = 0; epoch < epochs; ++epoch) {
            float errorSum = 0;
            for (size_t i = 0; i < X.size(); ++i) {
                float x[2] = {X[i][0], X[i][1]};
                float y, h[2];
                
                // Forward pass
                forward(x, y, h);
                
                // Calculate error
                float error = Y[i] - y;
                errorSum += error * error;
                
                // Backward propagation
                float d_y = error * sigmoid_derivative(y);
                
                for (int j = 0; j < 2; ++j) {
                    float d_h = d_y * w2[j] * sigmoid_derivative(h[j]);
                    w1[j][0] += learningRate * d_h * x[0];
                    w1[j][1] += learningRate * d_h * x[1];
                    b1[j] += learningRate * d_h;
                    w2[j] += learningRate * d_y * h[j];
                }
                b2 += learningRate * d_y;
            }

            if (epoch % 1000 == 0) {
                std::cout << "Epoch " << epoch << " Error: " << errorSum / X.size() << std::endl;
            }
        }
    }
};

// Sigmoid activation function
float sigmoid(float x) {
    return 1.0f / (1.0f + exp(-x));
}

// Derivative of sigmoid function for backpropagation
float sigmoid_derivative(float x) {
    return x * (1 - x);
}
